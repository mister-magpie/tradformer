{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37650e1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T11:13:39.210864Z",
     "start_time": "2021-11-26T11:13:37.970444Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431968e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T17:40:05.512703Z",
     "start_time": "2021-11-25T17:40:05.507417Z"
    }
   },
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e91f9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T11:13:39.242724Z",
     "start_time": "2021-11-26T11:13:39.212557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>R</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>K</th>\n",
       "      <th>abc</th>\n",
       "      <th>link</th>\n",
       "      <th>length</th>\n",
       "      <th>changekey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X:0</td>\n",
       "      <td>R:Slängpolska</td>\n",
       "      <td>L:1/16</td>\n",
       "      <td>M:3/4</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>G 2 F 2 E 4 C 4 | F 2 E 2 D 4 G, 4 | C 2 &gt; B, ...</td>\n",
       "      <td>http://www.folkwiki.se/Musik/251</td>\n",
       "      <td>182</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X:1</td>\n",
       "      <td>R:Polska</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>M:3/4</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>g &gt; f e 2 e 2 | f &gt; e d 2 G 2 | c &gt; B c d e f ...</td>\n",
       "      <td>http://www.folkwiki.se/Musik/251</td>\n",
       "      <td>127</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X:2</td>\n",
       "      <td>R:Slängpolska</td>\n",
       "      <td>L:1/16</td>\n",
       "      <td>M:3/4</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>C 2 &gt; E 2 G 2 A 2 G 2 E 2 | C 2 E 2 G 2 c 2 e ...</td>\n",
       "      <td>http://www.folkwiki.se/Musik/1972</td>\n",
       "      <td>294</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X:3</td>\n",
       "      <td>R:Slängpolska</td>\n",
       "      <td>L:1/16</td>\n",
       "      <td>M:3/4</td>\n",
       "      <td>K:Cmin</td>\n",
       "      <td>D 4 |: E F G 2 B, 4 B, 4 | C B, A, G, A, 4 G, ...</td>\n",
       "      <td>http://www.folkwiki.se/Musik/183</td>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X:4</td>\n",
       "      <td>R:Slängpolska</td>\n",
       "      <td>L:1/16</td>\n",
       "      <td>M:3/4</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>c 2 G 2 E G E C G, 2 A, B, | C 2 C E G 2 G B c...</td>\n",
       "      <td>http://www.folkwiki.se/Musik/2368</td>\n",
       "      <td>174</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>X:600</td>\n",
       "      <td>R:Slängpolska</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>M:3/4</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>g ^f /2 g /2 a g e c | (3 e f e d c d e | (3 f...</td>\n",
       "      <td>http://www.folkwiki.se/Musik/3137</td>\n",
       "      <td>330</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>X:601</td>\n",
       "      <td>R:Slängpolska</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>M:3/4</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>e 2 e f d f | e c c E G c | c B B d G B | B c ...</td>\n",
       "      <td>http://www.folkwiki.se/Musik/3138</td>\n",
       "      <td>311</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>X:602</td>\n",
       "      <td>R:Slängpolska</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>M:3/4</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>C B, C D D E | E C D E F 2 | E D E C E G | A /...</td>\n",
       "      <td>http://www.folkwiki.se/Musik/Vevlireettan</td>\n",
       "      <td>126</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>X:603</td>\n",
       "      <td>R:Slängpolska</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>M:3/4</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>G E F G A F | E F G 2 F C | F E F G A F | F /2...</td>\n",
       "      <td>http://www.folkwiki.se/Musik/Vevlire2an</td>\n",
       "      <td>117</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>X:604</td>\n",
       "      <td>R:Slängpolska</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>M:3/4</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>g 2 g a /2 g /2 f /2 e /2 d /2 c /2 | B /2 c /...</td>\n",
       "      <td>http://www.folkwiki.se/Musik/89</td>\n",
       "      <td>421</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>593 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X              R       L      M       K  \\\n",
       "0      X:0  R:Slängpolska  L:1/16  M:3/4  K:Cmaj   \n",
       "1      X:1       R:Polska   L:1/8  M:3/4  K:Cmaj   \n",
       "2      X:2  R:Slängpolska  L:1/16  M:3/4  K:Cmaj   \n",
       "3      X:3  R:Slängpolska  L:1/16  M:3/4  K:Cmin   \n",
       "4      X:4  R:Slängpolska  L:1/16  M:3/4  K:Cmaj   \n",
       "..     ...            ...     ...    ...     ...   \n",
       "600  X:600  R:Slängpolska   L:1/8  M:3/4  K:Cmaj   \n",
       "601  X:601  R:Slängpolska   L:1/8  M:3/4  K:Cmaj   \n",
       "602  X:602  R:Slängpolska   L:1/8  M:3/4  K:Cmaj   \n",
       "603  X:603  R:Slängpolska   L:1/8  M:3/4  K:Cmaj   \n",
       "604  X:604  R:Slängpolska   L:1/8  M:3/4  K:Cmaj   \n",
       "\n",
       "                                                   abc  \\\n",
       "0    G 2 F 2 E 4 C 4 | F 2 E 2 D 4 G, 4 | C 2 > B, ...   \n",
       "1    g > f e 2 e 2 | f > e d 2 G 2 | c > B c d e f ...   \n",
       "2    C 2 > E 2 G 2 A 2 G 2 E 2 | C 2 E 2 G 2 c 2 e ...   \n",
       "3    D 4 |: E F G 2 B, 4 B, 4 | C B, A, G, A, 4 G, ...   \n",
       "4    c 2 G 2 E G E C G, 2 A, B, | C 2 C E G 2 G B c...   \n",
       "..                                                 ...   \n",
       "600  g ^f /2 g /2 a g e c | (3 e f e d c d e | (3 f...   \n",
       "601  e 2 e f d f | e c c E G c | c B B d G B | B c ...   \n",
       "602  C B, C D D E | E C D E F 2 | E D E C E G | A /...   \n",
       "603  G E F G A F | E F G 2 F C | F E F G A F | F /2...   \n",
       "604  g 2 g a /2 g /2 f /2 e /2 d /2 c /2 | B /2 c /...   \n",
       "\n",
       "                                          link  length  changekey  \n",
       "0             http://www.folkwiki.se/Musik/251     182      False  \n",
       "1             http://www.folkwiki.se/Musik/251     127      False  \n",
       "2            http://www.folkwiki.se/Musik/1972     294      False  \n",
       "3             http://www.folkwiki.se/Musik/183      98      False  \n",
       "4            http://www.folkwiki.se/Musik/2368     174      False  \n",
       "..                                         ...     ...        ...  \n",
       "600          http://www.folkwiki.se/Musik/3137     330      False  \n",
       "601          http://www.folkwiki.se/Musik/3138     311      False  \n",
       "602  http://www.folkwiki.se/Musik/Vevlireettan     126      False  \n",
       "603    http://www.folkwiki.se/Musik/Vevlire2an     117      False  \n",
       "604            http://www.folkwiki.se/Musik/89     421      False  \n",
       "\n",
       "[593 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this dataframe is the result of the scraping procedure and contains tokenized \n",
    "slp_df = pd.read_pickle(\"./dataset/slangpolska_df_C.pickle\")\n",
    "slp_df['changekey'] = slp_df['abc'].apply(lambda x: len(re.findall('\\[?K:\\w+\\]?',x)) >= 1 )\n",
    "slp_df = slp_df[\n",
    "    (slp_df[\"changekey\"]==False) & # a few tunes have multiple key tokens, we discard them\n",
    "    (slp_df[\"length\"]<512) # we also discard a few tunes longer than 512 tokens\n",
    "               ]\n",
    "slp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "521adc4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T11:13:39.930773Z",
     "start_time": "2021-11-26T11:13:39.475556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OPENING:  ./dataset/data_v2.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183bb900b24744a6b3370591381896ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abc</th>\n",
       "      <th>K</th>\n",
       "      <th>M</th>\n",
       "      <th>length</th>\n",
       "      <th>L</th>\n",
       "      <th>tuplets</th>\n",
       "      <th>third</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G E E E 2 D E D C | G E E E F G A B c | G E E ...</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>M:9/8</td>\n",
       "      <td>166</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f B B c f B c c | f B B c a f e c | f B B c f ...</td>\n",
       "      <td>K:Cmin</td>\n",
       "      <td>M:4/4</td>\n",
       "      <td>103</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>|: c | f 3/2 a /2 c f 2 a | a 3/2 b /2 c' a 3/...</td>\n",
       "      <td>K:Cmix</td>\n",
       "      <td>M:6/8</td>\n",
       "      <td>301</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f &gt; a c f 2 a | a &gt; b c' a &gt; g f | e 2 g c &gt; e...</td>\n",
       "      <td>K:Cmix</td>\n",
       "      <td>M:6/8</td>\n",
       "      <td>253</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c &lt; e G c 2 G | e &gt; f g e d c | B &lt; d G B 2 d ...</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>M:6/8</td>\n",
       "      <td>248</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23630</th>\n",
       "      <td>G |: c 3 d 3 | e 3 e 2 d | e f g f e d | c 2 B...</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>M:6/8</td>\n",
       "      <td>364</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23631</th>\n",
       "      <td>G B | c 2 c _B G C E G | F 2 A F _B F A F | G ...</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>M:4/4</td>\n",
       "      <td>147</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23632</th>\n",
       "      <td>C D C C D E | F E F F G A | G 2 E C D E | D 2 ...</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>M:6/8</td>\n",
       "      <td>289</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23633</th>\n",
       "      <td>|: c' 2 c' b c' b | g 2 c' c' 2 b | g e e d e ...</td>\n",
       "      <td>K:Cmin</td>\n",
       "      <td>M:6/8</td>\n",
       "      <td>166</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23634</th>\n",
       "      <td>D E |: F 2 F G F E | D 2 D E F G | A 2 c 2 B 2...</td>\n",
       "      <td>K:Cmaj</td>\n",
       "      <td>M:3/4</td>\n",
       "      <td>88</td>\n",
       "      <td>L:1/8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23366 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     abc       K      M  \\\n",
       "0      G E E E 2 D E D C | G E E E F G A B c | G E E ...  K:Cmaj  M:9/8   \n",
       "1      f B B c f B c c | f B B c a f e c | f B B c f ...  K:Cmin  M:4/4   \n",
       "3      |: c | f 3/2 a /2 c f 2 a | a 3/2 b /2 c' a 3/...  K:Cmix  M:6/8   \n",
       "4      f > a c f 2 a | a > b c' a > g f | e 2 g c > e...  K:Cmix  M:6/8   \n",
       "5      c < e G c 2 G | e > f g e d c | B < d G B 2 d ...  K:Cmaj  M:6/8   \n",
       "...                                                  ...     ...    ...   \n",
       "23630  G |: c 3 d 3 | e 3 e 2 d | e f g f e d | c 2 B...  K:Cmaj  M:6/8   \n",
       "23631  G B | c 2 c _B G C E G | F 2 A F _B F A F | G ...  K:Cmaj  M:4/4   \n",
       "23632  C D C C D E | F E F F G A | G 2 E C D E | D 2 ...  K:Cmaj  M:6/8   \n",
       "23633  |: c' 2 c' b c' b | g 2 c' c' 2 b | g e e d e ...  K:Cmin  M:6/8   \n",
       "23634  D E |: F 2 F G F E | D 2 D E F G | A 2 c 2 B 2...  K:Cmaj  M:3/4   \n",
       "\n",
       "       length      L  tuplets  third  \n",
       "0         166  L:1/8    False  False  \n",
       "1         103  L:1/8    False  False  \n",
       "3         301  L:1/8    False  False  \n",
       "4         253  L:1/8    False  False  \n",
       "5         248  L:1/8    False  False  \n",
       "...       ...    ...      ...    ...  \n",
       "23630     364  L:1/8    False  False  \n",
       "23631     147  L:1/8    False  False  \n",
       "23632     289  L:1/8    False  False  \n",
       "23633     166  L:1/8    False  False  \n",
       "23634      88  L:1/8    False  False  \n",
       "\n",
       "[23366 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./dataset/data_v2.txt\"\n",
    "with open(path,\"r\",encoding=\"utf-8\") as f:\n",
    "    print(\"\\nOPENING: \",path)\n",
    "    data = f.read()\n",
    "\n",
    "    data = data.replace('/2<','/2 <')\n",
    "    data = data.replace('/2>','/2 >')\n",
    "    data = data.replace('2<','2 <')\n",
    "    data = data.replace('2>','2 >')\n",
    "    data = data.replace('3<','3 <')\n",
    "    data = data.replace('3>','3 >')\n",
    "    data = data.replace('4<','4 <')\n",
    "    data = data.replace('4>','4 >')\n",
    "    data = data.replace('=b=B','=b =B')\n",
    "    \n",
    "    TOKENS_V2 = sorted(list(set(data.split())))\n",
    "    TOKENS_V2 = ['<pad>','<s>'] + TOKENS_V2 + ['</s>']\n",
    "    data = data.split(\"\\n\\n\")\n",
    "\n",
    "    pieces = []\n",
    "    keys = []\n",
    "    meters = []\n",
    "    lengths = []\n",
    "    \n",
    "    for d in tqdm(data[:-1]): #there's a trailing newline\n",
    "\n",
    "        m,k,p = d.split('\\n')\n",
    "        l = len(p.split())\n",
    "        \n",
    "        pieces.append(p)\n",
    "        keys.append(k)\n",
    "        meters.append(m)\n",
    "        lengths.append(l)\n",
    "        \n",
    "    df_v2 = pd.DataFrame.from_dict({\n",
    "        'piece':pieces,\n",
    "        'mode':keys,\n",
    "        'meter':meters,\n",
    "        'length':lengths\n",
    "    })\n",
    "    df_v2['L'] = 'L:1/8'\n",
    "\n",
    "df_v2['tuplets'] = df_v2['piece'].apply(lambda x: len(re.findall('\\([245679]',x)) > 0 )\n",
    "df_v2['third'] = df_v2['piece'].apply(lambda x: len(re.findall('/3',x)) > 0 )\n",
    "df_v2 = df_v2[\n",
    "    (df_v2[\"tuplets\"]==False) &\n",
    "    (df_v2[\"third\"]==False) &\n",
    "    (df_v2[\"length\"]<512)\n",
    "               ].rename(columns={'meter':'M','mode':'K','piece':'abc',})\n",
    "\n",
    "df_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37987222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T11:13:40.317953Z",
     "start_time": "2021-11-26T11:13:39.932204Z"
    }
   },
   "outputs": [],
   "source": [
    "TOKENS_SLP = set([])\n",
    "_ = slp_df.abc.apply(lambda x: [TOKENS_SLP.add(tt) for tt in x.split(' ')])\n",
    "TOKENS_V2 = set([])\n",
    "_ = df_v2.abc.apply(lambda x: [TOKENS_V2.add(tt) for tt in x.split(' ')])\n",
    "\n",
    "#TOKENS_SLP\n",
    "TOKENS = TOKENS_SLP.union(TOKENS_V2)\n",
    "\n",
    "for tt in ['<s>','</s>','L:1/8', 'L:1/16','M:12/8', 'M:2/4', 'M:3/2', 'M:3/4', 'M:4/4', 'M:6/8', 'M:9/8','K:Cdor', 'K:Cmaj', 'K:Cmin', 'K:Cmix','<pad>']:\n",
    "    TOKENS.add(tt)\n",
    "    \n",
    "TOKENS.remove('')\n",
    "TOKENS = sorted(list(TOKENS))\n",
    "assert len(TOKENS) == 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de363c6",
   "metadata": {},
   "source": [
    "### datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e59fae6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T11:13:40.959338Z",
     "start_time": "2021-11-26T11:13:40.610815Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "  \n",
    "class TokenDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, block_size, TOKENS):\n",
    "        self.dataset = dataset\n",
    "        data_size, vocab_size = len(self.dataset ), len(TOKENS)\n",
    "        print('data has %d pieces, %d unique tokens.' % (data_size, vocab_size))\n",
    "        self.stoi = { tk:i for i,tk in enumerate(TOKENS) }\n",
    "        self.itos = { i:tk for i,tk in enumerate(TOKENS) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.dataset[idx:idx+1]\n",
    "        # encode every token to an integer\n",
    "        dix = [self.stoi[s] for s in chunk[0]]\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generateDatasetSplit(df,max_len=512, split=0.05):\n",
    "    # max_len - 5 because we need to SOS + EOS + L,M,K headers\n",
    "    df = df[df.length <= max_len-5] \n",
    "    df = '<s> ' + df['L'].map(str) + '\\n' + df['M'].map(str) + '\\n' + df['K'].map(str) + '\\n' + df['abc'].map(str) + ' </s>'\n",
    "    df = df.values.reshape(-1,1)\n",
    "\n",
    "    #takes a numpy array as input\n",
    "    def padding(array,max_len):\n",
    "        array = array[0].split()\n",
    "        array = np.append(array,['<pad>']*(max_len-len(array) ))\n",
    "        assert len(array) == max_len\n",
    "        return np.array(array)\n",
    "\n",
    "    dataset = np.asarray([padding(x,max_len) for x in tqdm(df[:])])\n",
    "    \n",
    "    if split:\n",
    "        return train_test_split(dataset,test_size=split,random_state=1080)\n",
    "    else:\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48cade0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T11:13:43.372365Z",
     "start_time": "2021-11-26T11:13:40.977825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6fe860bb7c42a49c56d9a0b6366c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22193, 512) (1169, 512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45ab8a5aa1744c9974a238f23a72349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(593, 512)\n"
     ]
    }
   ],
   "source": [
    "train_split,test_split = generateDatasetSplit(df_v2)\n",
    "print(train_split.shape,test_split.shape)\n",
    "\n",
    "finetune_data = generateDatasetSplit(slp_df,split=None)\n",
    "print(finetune_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e957a66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T11:13:43.419445Z",
     "start_time": "2021-11-26T11:13:43.375228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 22193 pieces, 128 unique tokens.\n",
      "data has 1169 pieces, 128 unique tokens.\n",
      "data has 593 pieces, 128 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "block_size = 512 # spatial extent of the model for its context\n",
    "\n",
    "train_dataset = TokenDataset(train_split, block_size,TOKENS)\n",
    "test_dataset = TokenDataset(test_split, block_size, TOKENS)\n",
    "finetune_dataset = TokenDataset(finetune_data, block_size, TOKENS)\n",
    "\n",
    "assert train_dataset.itos == test_dataset.itos\n",
    "assert train_dataset.stoi == test_dataset.stoi\n",
    "assert finetune_dataset.stoi == test_dataset.stoi\n",
    "assert finetune_dataset.stoi == test_dataset.stoi\n",
    "\n",
    "#TOKENS\n",
    "df_v2.to_pickle(\"./dataset/df_v4.pickle\")\n",
    "slp_df.to_pickle(\"./dataset/df_v4_finetune.pickle\")\n",
    "np.save(\"./dataset/TOKENS_V4.pickle\",TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68a8ca47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T11:13:43.463186Z",
     "start_time": "2021-11-26T11:13:43.421853Z"
    }
   },
   "outputs": [],
   "source": [
    "# model spec and trainer function\n",
    "\n",
    "# make deterministic\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class GPTConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class GPT1Config(GPTConfig):\n",
    "    \"\"\" GPT-1 like network roughly 125M params \"\"\"\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    n_embd = 768\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=256):\n",
    "        super().__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "    \n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        \n",
    "        # for visualization \n",
    "        self.att_weights = None\n",
    "        \n",
    "    def forward(self, x, layer_past=None):\n",
    "        # B is the batch size, \n",
    "        # T is the sequence length, \n",
    "        # C is the dimensionality of the embedding (n_embd).\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / np.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        # for plotting\n",
    "        self.att_weights = att\n",
    "        \n",
    "        # att = self.attn_drop(att)\n",
    "        y = self.attn_drop(att) @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        \n",
    "        # input embedding with padding\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd, \n",
    "                                    padding_idx = finetune_dataset.stoi['<pad>'])\n",
    "        #self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.pos_emb = PositionalEmbedding(config.n_embd, self.block_size)\n",
    "        \n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        \n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=True) # bias added for comparison with folkrnn\n",
    "\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        #no_decay.add('pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "        # forward the GPT model\n",
    "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
    "        #position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "        position_embeddings = self.pos_emb(token_embeddings)\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\"\"\"\n",
    "Simple training loop; Boilerplate that could apply to any arbitrary neural network,\n",
    "so nothing in this file really has anything to do with GPT specifically.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    " \n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_dataset, test_dataset, config, writer=None):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.config = config\n",
    "        self.summaryWriter = writer\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path)\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            data = self.train_dataset if is_train else self.test_dataset\n",
    "            loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
    "                                batch_size=config.batch_size,\n",
    "                                num_workers=config.num_workers)\n",
    "\n",
    "            losses = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, (x, y) in pbar:\n",
    "\n",
    "                # place data on the correct device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                if is_train:\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    self.summaryWriter.add_scalar(\"batchLoss\", loss.item(),(len(loader)*epoch)+it)\n",
    "                     \n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
    "            \n",
    "            if is_train:\n",
    "                #print(len(losses))\n",
    "                train_loss = float(np.mean(losses))\n",
    "                if self.summaryWriter:\n",
    "                    self.summaryWriter.add_scalar(\"epochLoss\", train_loss, epoch)\n",
    "                logger.info(\"epoch train loss: %f\", train_loss)\n",
    "                return train_loss\n",
    "            \n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                if self.summaryWriter:\n",
    "                    self.summaryWriter.add_scalar(\"valLoss\", test_loss, epoch)\n",
    "                logger.info(\"test loss: %f\", test_loss)\n",
    "                return test_loss\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "\n",
    "        for epoch in range(config.max_epochs):\n",
    "            train_loss = run_epoch('train')\n",
    "            \n",
    "\n",
    "            if self.test_dataset is not None:\n",
    "                test_loss = run_epoch('test')\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            good_model = self.test_dataset is None or test_loss < best_loss\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                if self.test_dataset is not None:\n",
    "                    best_loss = test_loss\n",
    "                    self.save_checkpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5447b01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T11:13:43.468810Z",
     "start_time": "2021-11-26T11:13:43.464810Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "#CKPT = \"session_model_ckpt_60\" #8l 1h layers\n",
    "#CKPT_NAME = \"session_model_164_ckpt_60\" #8l 1h layers\n",
    "\n",
    "def save_model(CKPT_NAME,model):\n",
    "    # SAVE THE SESSION MODEL \n",
    "    # DataParallel wrappers keep raw model object in .module attribute\n",
    "    raw_model = model.module if hasattr(model, \"module\") else model\n",
    "    torch.save(raw_model.state_dict(), CKPT_NAME)\n",
    "    print('Checkpoint saved!',CKPT_NAME)\n",
    "    \n",
    "def load_model(CKPT_NAME,model):\n",
    "    ckpt_model = model.module if hasattr(model, \"module\") else model\n",
    "    try:\n",
    "        ck = torch.load(CKPT_NAME)\n",
    "    except:\n",
    "        return None\n",
    "    ckpt_model.load_state_dict(copy.deepcopy(ck))\n",
    "    model.cuda()\n",
    "    print('Checkpoint loaded!',CKPT_NAME)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2efab0ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T11:13:45.854917Z",
     "start_time": "2021-11-26T11:13:43.608660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699c4e8e6ad8400ab54c25dc23975e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 22193 pieces, 128 unique tokens.\n",
      "data has 1169 pieces, 128 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# PATHS\n",
    "DF_PATH = './dataset/df_v4.pickle'\n",
    "DF_FINETUNE_PATH = './dataset/df_v4_finetune.pickle'\n",
    "TOKENS = np.load('./dataset/TOKENS_V4.pickle.npy',allow_pickle=True)\n",
    "\n",
    "# DATASET\n",
    "block_size = 512\n",
    "train_split,test_split = generateDatasetSplit(pd.read_pickle(DF_PATH))\n",
    "\n",
    "train_dataset = TokenDataset(train_split, block_size,TOKENS)\n",
    "test_dataset = TokenDataset(test_split, block_size,TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60d92f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T11:13:50.637610Z",
     "start_time": "2021-11-26T11:13:50.539078Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/26/2021 12:13:50 - INFO - __main__ -   number of parameters: 3.205504e+06\n"
     ]
    }
   ],
   "source": [
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=16, n_head=4, n_embd=128)\n",
    "session_model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbf1c731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T11:13:53.579067Z",
     "start_time": "2021-11-26T11:13:51.598779Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/26/2021 12:13:51 - INFO - __main__ -   number of parameters: 3.205504e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded! ./models/session_model_164_ckpt_60\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime as dt\n",
    "\n",
    "CKPT_NAME = \"./models/session_model_164_ckpt_60\" \n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=16, n_head=4, n_embd=128)\n",
    "session_model = GPT(mconf)\n",
    "session_model = load_model(CKPT_NAME,session_model)\n",
    "\n",
    "if session_model is None:\n",
    "    session_model = GPT(mconf)\n",
    "    tconf = TrainerConfig(max_epochs=1, batch_size=64,\n",
    "                          learning_rate=5e-3,\n",
    "                          lr_decay=False, #warmup_tokens=256*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                          num_workers=8, \n",
    "                          #ckpt_path = './' + str(dt.datetime.now().strftime(\"%b%d_%H-%M-%S\")) + '_ckpt'\n",
    "                         )\n",
    "    #writer = SummaryWriter(log_dir='./runs/slangpolska/'+str(dt.datetime.now().strftime(\"%b%d_%H-%M-%S\")) )\n",
    "    trainer = Trainer(session_model, train_dataset, test_dataset,tconf, writer=None)\n",
    "    trainer.train()\n",
    "    #writer.flush()\n",
    "    #writer.add_text(\"text\",\"pretraining slangpolska\",0)\n",
    "    #writer.flush()\n",
    "    CKPT_NAME = \"./models/session_model_164_ckpt_60\" #8l 1h layers\n",
    "    save_model(CKPT_NAME,session_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58a62021",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T13:33:36.487882Z",
     "start_time": "2021-11-26T13:33:36.465264Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] =  -float('Inf')\n",
    "    return out\n",
    "\n",
    "def top_p_logits(logits, top_p,verbose=False):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    # Remove tokens with cumulative probability above the threshold\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    # Shift the indices to the right to keep also the first token above the threshold\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    out = logits.clone()\n",
    "    indices_to_remove = torch.zeros_like(out,dtype=torch.bool).scatter_(dim=-1, index=sorted_indices, src=sorted_indices_to_remove )\n",
    "    out[indices_to_remove] = -float('Inf')#filter_value\n",
    "    if verbose: \n",
    "        print('|top_p:',(out > -float('Inf')).sum().item(),'\\n')\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, temperature=1.0, sample=True, top_k=None, top_p=None):\n",
    "\n",
    "    block_size = model.get_block_size()\n",
    "    model.eval()\n",
    "    #print(x.shape)\n",
    "    l = torch.zeros((1,model.tok_emb.weight.size()[0])).to('cuda')\n",
    "    p = torch.zeros((1,model.tok_emb.weight.size()[0])).to('cuda')\n",
    "\n",
    "    for k in range(steps):\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "        \n",
    "        logits, _ = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None and top_k > 0.0:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # optionally crop probabilities to only the top p probability mass\n",
    "        if top_p is not None and top_p > 0.0:\n",
    "            logits = top_p_logits(logits, top_p)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        else: probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        \n",
    "        # append to the sequence and continue\n",
    "        #print(l.shape)\n",
    "        l = torch.cat((l, logits), dim=0)\n",
    "        p = torch.cat((p, probs), dim=0)\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "        if ix == train_dataset.stoi[\"</s>\"] or ix == train_dataset.stoi[\"<pad>\"]:\n",
    "            #print('end token reached')\n",
    "            break\n",
    "\n",
    "    return x,l, p\n",
    "\n",
    "@torch.no_grad() \n",
    "def recursiveBeam(model,context_tokens, context_logprob, depth, P1=.99, K1=2, T1=1.2, verbose=True,):\n",
    "    model.eval()\n",
    "    if verbose: print('\\nrecursiveBeam depth:',depth,'\\ninput',context_tokens)\n",
    "\n",
    "    logits, _ = model(context_tokens)\n",
    "    logits = logits[:, -1, :] / T1 # logits for last step\n",
    "    logits = top_p_logits(logits, P1)\n",
    "    probs = F.softmax(logits, dim=-1).detach()\n",
    "    candidate_tokens = torch.where(probs > 0.0)[1].reshape(1,-1) \n",
    "    candidate_logprobs = torch.log(probs[0,candidate_tokens])\n",
    "    \n",
    "    if candidate_tokens.shape[1] > K1:\n",
    "        if verbose: print('too many',candidate_tokens.shape[1])\n",
    "        candidate_logprobs, candidate_tokens = torch.topk(torch.log(probs), K1)    \n",
    "    \n",
    "    if verbose: print('tok', candidate_tokens, '\\nprobs',candidate_logprobs)\n",
    "    if verbose: print(\"branches:\",candidate_tokens.shape[1])\n",
    "    # return datastructure\n",
    "    ret_T = []\n",
    "    ret_P = []\n",
    "    \n",
    "    # iterate over the candidate tokens\n",
    "    for i in range(0,candidate_tokens.shape[1]):\n",
    "        t = torch.cat([context_tokens , candidate_tokens[0,i].reshape(1,1)],dim=1)\n",
    "        p = context_logprob + candidate_logprobs[0,i].reshape(1,1)\n",
    "        if verbose: print(i,'\\tt', t,'\\tp',p)\n",
    "        # base case append the results\n",
    "        if depth==0:\n",
    "            ret_T.append(t)\n",
    "            ret_P.append(p)     \n",
    "        # recursive call, then append the results\n",
    "        # complete tokens sequences are propagated from the bottom back\n",
    "        else:\n",
    "            rt,rp = recursiveBeam(model,t, p, depth-1, \n",
    "                                  K1=K1, P1=P1, T1=T1,\n",
    "                                  verbose=verbose)\n",
    "            ret_T.append(rt)\n",
    "            ret_P.append(rp)\n",
    "            \n",
    "    RT,RP = torch.cat(ret_T,dim=0), torch.cat(ret_P,dim=0)     \n",
    "    return RT,RP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1679f876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T13:33:38.535866Z",
     "start_time": "2021-11-26T13:33:38.524576Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def simple_sample(model, SEED, verbose=False,print_score=False):\n",
    "    if verbose: print('SEED:',SEED)\n",
    "    set_seed(SEED)\n",
    "\n",
    "    prompt = \"<s> L:1/16 M:3/4 \" + np.random.choice(['K:Cmaj','K:Cmin'])\n",
    "    x = torch.tensor([train_dataset.stoi[s] for s in prompt.split()], dtype=torch.long)[None,...].to(\"cuda\")\n",
    "    y,l,p = sample(model, x, 512, temperature=1.0 , top_p=0.99, sample=True)\n",
    "    if verbose: print(y)\n",
    "    completion = ''.join([train_dataset.itos[int(i)] for i in y[0]])\n",
    "\n",
    "    end = completion.find('</s>')\n",
    "    if verbose: print(completion)\n",
    "\n",
    "    if end == -1:\n",
    "        if verbose: print(\"not finished\")\n",
    "        return None\n",
    "    else:\n",
    "        abc = completion[3:end]\n",
    "        abc = abc[:6] + \"\\n\" + abc[6:]\n",
    "        abc = abc[:12] + \"\\n\" + abc[12:]\n",
    "        abc = abc[:19] + \"\\n\" + abc[19:]\n",
    "\n",
    "        abc = 'X:'+str(SEED)+'\\nT:'+str(SEED)+'\\nQ:1/4=100\\n' + abc\n",
    "\n",
    "        if verbose: print(abc,\"\\n\")\n",
    "        if print_score:\n",
    "            #replacement for javascript\n",
    "            abc_score = abc.replace('\\n','\\\\n')\n",
    "            abc_score = abc_score.replace(':|' ,':|\\\\n')\n",
    "            abc_score = abc_score.replace(':||:',':|\\\\n|:')\n",
    "            abcPlayer(abc_score)#'\\\"'+abc+'\\\"')\n",
    "        return abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb4ff734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T13:34:14.451965Z",
     "start_time": "2021-11-26T13:34:14.434500Z"
    }
   },
   "outputs": [],
   "source": [
    "## generate some sample with beamsearch\n",
    "def beamsearch_sample(model,SEED,verbose=False,print_score=False):\n",
    "    if verbose: print('SEED:',SEED)\n",
    "    set_seed(SEED)\n",
    "\n",
    "    prompt = \"<s> L:1/16 M:3/4 \" + np.random.choice(['K:Cmaj','K:Cmin'])\n",
    "    x = torch.tensor([train_dataset.stoi[s] for s in prompt.split()], dtype=torch.long)[None,...].to('cuda')\n",
    "    # print('prompt:',x)\n",
    "    D = 3\n",
    "    K1 = 2\n",
    "    P1=.99\n",
    "    T1 = 1.15 \n",
    "    T3 = 1.15 \n",
    "    P3 = 0.99 \n",
    "    \n",
    "    if verbose: pbar = tqdm(initial=x.shape[1],total=512)\n",
    "\n",
    "    while x.shape[1] < 512-(D+1):\n",
    "        if (x[0,-1] == train_dataset.stoi['</s>']) | (x[0,-1] == train_dataset.stoi['<pad>']):break\n",
    "        if verbose: pbar.update(D+1)\n",
    "        T,P = recursiveBeam( model, x, torch.FloatTensor([0.0]).to(\"cuda\") , K1=K1, P1=P1, T1=T1, depth=D,verbose=False)\n",
    "        P = P.reshape(1,-1) / T3   \n",
    "        P = top_p_logits(P, P3,verbose=False)\n",
    "        P = F.softmax(P,dim=1)\n",
    "        ix2 = torch.multinomial(P,num_samples=1)\n",
    "        x = T[ix2].reshape(1,-1)\n",
    "\n",
    "    if verbose: pbar.close()\n",
    "    y = x.detach().cpu().numpy()\n",
    "    if verbose: print(y)\n",
    "        \n",
    "    # avoid hanged repetitions\n",
    "    for i in reversed(np.where(y[0]==18)[0][:-1]):\n",
    "        if y[0,i+1] != 127: \n",
    "            y = np.insert(y,i+1,127,axis=1)\n",
    "\n",
    "    try:\n",
    "        end = 1 + np.where( (y==train_dataset.stoi['</s>']) | (y==train_dataset.stoi['<pad>']) )[1][0] \n",
    "    except IndexError:\n",
    "        if verbose: print('no end')\n",
    "        return None\n",
    "\n",
    "    #format string\n",
    "    abc = ''.join([train_dataset.itos[int(i)] for i in y[0][1:end-1]])\n",
    "    abc = abc[0:6]+'\\n' + abc[6:11]+'\\n'+abc[11:17]+'\\n'+abc[17:]\n",
    "    abc = 'X:'+str(SEED)+'\\nT:'+str(SEED)+'\\nC:tradformer-ft-depthbeam\\nQ:1/4=100\\n'+abc\n",
    "    if verbose: print(abc)\n",
    "        \n",
    "    if print_score:\n",
    "        if verbose: \n",
    "            print(abc,\"\\n\")\n",
    "        abc = abc.replace(':||:',':|\\n|:')\n",
    "        abcPlayer(abc,H=450)\n",
    "    return abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "758a2a12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T13:32:33.392904Z",
     "start_time": "2021-11-26T13:32:33.292239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6507028fef9440e826989520f458022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 593 pieces, 128 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "finetune_data = generateDatasetSplit(pd.read_pickle(DF_FINETUNE_PATH),split=None)\n",
    "finetune_dataset = TokenDataset(finetune_data, block_size, TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a6eaabd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T13:23:39.123635Z",
     "start_time": "2021-11-26T13:23:39.025850Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/26/2021 14:23:39 - INFO - __main__ -   number of parameters: 3.205504e+06\n"
     ]
    }
   ],
   "source": [
    "CKPT_NAME = \"session_model_164_ckpt_60\" #8l 1h layers\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=16, n_head=4, n_embd=128)\n",
    "slp_model = GPT(mconf)\n",
    "slp_model = load_model(CKPT_NAME,slp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee229ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tconf = TrainerConfig(max_epochs=30, batch_size=64,\n",
    "                      learning_rate=5e-4,\n",
    "                      lr_decay=False, #warmup_tokens=256*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=6, \n",
    "                      #ckpt_path = './' + str(dt.datetime.now().strftime(\"%b%d_%H-%M-%S\")) + '_ckpt'\n",
    "                     )\n",
    "#writer = SummaryWriter(log_dir='./runs/slangpolska/')\n",
    "trainer = Trainer(slp_model, finetune_dataset, None,tconf, writer=None)\n",
    "trainer.train()\n",
    "#writer.flush()\n",
    "\n",
    "# writer.add_text(\"text\",\"finetuning slangpolska\",0)\n",
    "# writer.flush()\n",
    "\n",
    "CKPT_NAME = \"./models/slp_model_164_ckpt_30\" #8l 1h layers\n",
    "save_model(CKPT_NAME,slp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99febdd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T13:25:25.691356Z",
     "start_time": "2021-11-26T13:25:25.432024Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/26/2021 14:25:25 - INFO - __main__ -   number of parameters: 3.205504e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded! ./models/slp_model_164_ckpt_30\n"
     ]
    }
   ],
   "source": [
    "CKPT_NAME = \"./models/slp_model_164_ckpt_30\" #8l 1h layers\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=16, n_head=4, n_embd=128)\n",
    "slp_model = GPT(mconf)\n",
    "slp_model = load_model(CKPT_NAME,slp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36fd5b9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T13:29:49.774068Z",
     "start_time": "2021-11-26T13:29:49.769722Z"
    }
   },
   "outputs": [],
   "source": [
    "# _ = simple_sample(slp_model,123,verbose=True,print_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "509b43bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T13:34:25.094015Z",
     "start_time": "2021-11-26T13:34:18.151746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3392da54e104983a0539ba08d09ec57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  1%|          | 4/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22  65  70  62 113   6 113  49 113   6 113  49 113   6 115 117 124 119\n",
      "  117 119 115 117   6 117 113 115   6 115  49 124 113  49 113 117 115 113\n",
      "   49  47  59   6  57   6 124  55   6  53   6  51   6  50   6  48   6  60\n",
      "    6 124  51   6  51  50  51   6  51  50  51   6  53  55 124  57  55  57\n",
      "   53  55   6 117 113 115   6 115  49 124 113  49 113 117 115 113  49  47\n",
      "   59   6  57   6 124  55   6  53   6  51   6  50   6  51  10  18 127  59\n",
      "   47  49 113 115 113  49 113 117 113  49 113 124 115 113  49  47  59  47\n",
      "   49 113 115   6 115  49 124 113  49 113 117 115 113  49  47  59   6  57\n",
      "    6 124  55   6  53   6  51   6  50   6  48   6  60   6 124  59  47  49\n",
      "  113 115 113  49 113 117 113  49 113 124 115 113  49  47  59  47  49 113\n",
      "  115   6 115  49 124 113  49 113 117 115 113  49  47  59   6  57   6 124\n",
      "   55   6  53   6  51   6  50   6  51  10  18  20  21  21]]\n",
      "X:123\n",
      "T:123\n",
      "C:tradformer-ft-depthbeam\n",
      "Q:1/4=100\n",
      "L:1/16\n",
      "M:3/4\n",
      "K:Cmaj\n",
      "c2cBc2cBc2de|fefde2ecd2dB|cBcedcBAG2F2|E2D2C2B,2A,2G,2|C2CB,C2CB,C2DE|FEFDE2ecd2dB|cBcedcBAG2F2|E2D2C2B,2C4:||:GABcdcBcecBc|dcBAGABcd2dB|cBcedcBAG2F2|E2D2C2B,2A,2G,2|GABcdcBcecBc|dcBAGABcd2dB|cBcedcBAG2F2|E2D2C2B,2C4:|\n"
     ]
    }
   ],
   "source": [
    "_ = beamsearch_sample(slp_model,123,verbose=True,print_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cddd6848",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T13:25:53.374281Z",
     "start_time": "2021-11-26T13:25:53.360200Z"
    }
   },
   "outputs": [],
   "source": [
    "def max_repeating_bars(abc):\n",
    "    bars = re.findall('[^||\\|:]+[\\||(:\\|)]',abc[70:])\n",
    "    return max([len(re.findall(re.escape(bar),abc)) for bar in bars])\n",
    "\n",
    "# o = beamsearch_sample(1038,verbose=True,print_score=True)\n",
    "# max_repeating_bars(o)\n",
    "\n",
    "NOTE_REGEX = \"[\\^\\_\\=]?[ABCDEFG][,]?|[\\^\\_\\=]?[abcdefg][']?|z\"\n",
    "NOTE_NUMBER_REGEX = \"({NOTE})(?=\\d\\d?)\".format(NOTE=NOTE_REGEX)#\"(?<={NOTE})[\\d][\\d]?(?={NOTE})\".format(NOTE=NOTE_REGEX)\n",
    "#CHORD_NUMBER_REGEX = \"[({NOTE})(?=\\d\\d?)\".format(NOTE=NOTE_REGEX)\n",
    "#print(NOTE_NUMBER_REGEX)\n",
    "def chord_number_sub(matchobj):\n",
    "    #print(matchobj)\n",
    "    s = re.findall('\\d',matchobj.group())[0]\n",
    "   # print(s)\n",
    "    return s\n",
    "    \n",
    "def couple_repeating_bars(abc):\n",
    "    bars = re.findall('[^||\\|:]+\\|[^||\\|:]+[\\||(:\\|)]',abc[70:])\n",
    "    #rb = sorted([len(re.findall(re.escape(bar),abc)) for bar in bars],reverse=True)\n",
    "    return max([len(re.findall(re.escape(bar),abc)) for bar in bars])\n",
    "    \n",
    "def check_bars(abc,verbose=False):\n",
    "    bars = re.findall('[^||\\|:|\\|1]+[\\||(:\\|)]',abc[70:])\n",
    "    for b in bars:\n",
    "        try:\n",
    "            b = b.strip(':|')\n",
    "            if verbose: print(b)\n",
    "            b = re.sub(\"([\\^\\_\\=]?[ABCDEFG][,]?|[\\^\\_\\=]?[abcdefg][']?)2\\>([\\^\\_\\=]?[ABCDEFG][,]?|[\\^\\_\\=]?[abcdefg][']?)2\",'4',b)\n",
    "            b = re.sub(NOTE_NUMBER_REGEX,' ',b)\n",
    "            b = re.sub(NOTE_REGEX,' 1 ',b) # notes regex\n",
    "            b = re.sub('\\[[\\s\\d]+\\]',chord_number_sub,b)\n",
    "            if verbose: print(b)\n",
    "            s = sum([int(n) for n in b.split()])\n",
    "            if s != 12: return 0\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return 2\n",
    "    return 1\n",
    "\n",
    "# prova = output_df.loc[13].abc\n",
    "# print(prova)\n",
    "# check_bars(prova,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f822ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append new outputs\n",
    "with open(\"./outputs/slangpolska_1000_beam_ht.txt\",\"w\") as o:\n",
    "    LEN = 103 # mrp <= 4\n",
    "    c = 1+1324 # starting from\n",
    "    tot = 0\n",
    "    pbar = tqdm(initial=0,total=LEN)\n",
    "    while tot < LEN:\n",
    "        abc = beamsearch_sample(c)\n",
    "\n",
    "        if abc != None:\n",
    "            # check goodness\n",
    "            mrp = max_repeating_bars(abc)\n",
    "            if  mrp > 4:\n",
    "                print(c,\"too many repetitions\")\n",
    "                c += 1\n",
    "                continue\n",
    "            # check goodness\n",
    "            crp = couple_repeating_bars(abc)\n",
    "            if crp > 4:\n",
    "                print(c,\"too many couple repetitions\")\n",
    "                c += 1\n",
    "                continue\n",
    "            # check goodness  \n",
    "            if check_bars(abc) != 1:\n",
    "                print(c,\"uneven bars\")\n",
    "                c += 1\n",
    "                continue\n",
    "            else:\n",
    "                o.write(abc+\"\\n\\n\")\n",
    "                pbar.update(1)\n",
    "                c += 1\n",
    "                tot += 1\n",
    "        else:\n",
    "            print(c,\"no end\")\n",
    "            c += 1\n",
    "    pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
